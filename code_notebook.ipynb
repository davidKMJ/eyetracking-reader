{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import cv2\n",
    "import dlib\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상수 정의\n",
    "SCREEN_WIDTH = 1280 # 화면 너비\n",
    "SCREEN_HEIGHT = 720 # 화면 높이\n",
    "BUTTON_WIDTH = 300 # 버튼 너비\n",
    "BUTTON_HEIGHT = 100 # 버튼 높이\n",
    "BUTTON_COLOR = (100, 100, 100) # 버튼 색상\n",
    "BUTTON_COLOR_HOVER = (150, 150, 150) # 버튼 호버 색상\n",
    "TEXT_COLOR = (255, 255, 255) # 텍스트 색상\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX # 폰트\n",
    "FONT_SCALE = 1.5 # 버튼 폰트 크기\n",
    "FONT_THICKNESS = 3 # 버튼 폰트 두께1\n",
    "FONT_THICKNESS_SLIM = 2 # 버튼 폰트 두께2\n",
    "LINE_TYPE = cv2.LINE_AA # 선 종류\n",
    "WINDOW_NAME = \"READING HELPER\" # 창 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\idfor\\miniconda3\\envs\\CSP\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드, 기본 데이터, 파라미터 설정 (고민재)\n",
    "# 모델 학습 과정은 개별 로그 코드로 따로 제출\n",
    "detector = dlib.get_frontal_face_detector() # 얼굴 검출기\n",
    "predictor = dlib.shape_predictor('shape_68.dat') # 얼굴 landmark 검출기\n",
    "\n",
    "left = [36, 37, 38, 39, 40, 41] # 왼쪽 눈 landmark\n",
    "right = [42, 43, 44, 45, 46, 47] # 오른쪽 눈 landmark\n",
    "kernel = np.ones((9, 9), np.uint8) # 커널\n",
    "threshold = 0 # 동공 검출 threshold\n",
    "\n",
    "model = tf.keras.models.load_model('default_model.keras') # default eye-tracking 모델 (미리 학습시켜 놓음)\n",
    "\n",
    "# default landmark 불러오기\n",
    "landmarks = None\n",
    "with open('default_landmarks.pkl', 'rb') as file:\n",
    "    landmarks = pickle.load(file)\n",
    "\n",
    "\n",
    "# default 보정 데이터 불러오기\n",
    "default_eye_datas = None\n",
    "default_screen_positions = None\n",
    "with open('default_calibration_data.pkl', 'rb') as file:\n",
    "    default_eye_datas, default_screen_positions = pickle.load(file)\n",
    "\n",
    "# 영상 처리 변수\n",
    "cap = None\n",
    "\n",
    "# 보정 데이터 생성 과정 파라미터\n",
    "horizontal_number = 3\n",
    "vertical_number = 3\n",
    "\n",
    "eye_datas = []\n",
    "screen_positions = []\n",
    "\n",
    "# 매끄러운 움직임 처리 파라미터\n",
    "num = 3\n",
    "std = 1.5\n",
    "alpha = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 읽기 관련 함수 및 파라미터 (임지윤)\n",
    "# 영어 텍스트 파일 읽기\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# 두 문장씩 나누기\n",
    "def split_text(text):\n",
    "    # 문장 구분\n",
    "    sent_end = re.compile(r'(?<=[.!?]) +')\n",
    "    sent = sent_end.split(text)\n",
    "    \n",
    "    # 두 문장씩 묶기\n",
    "    group_sent = []\n",
    "    for i in range(0, len(sent), 2):\n",
    "        if i+1 < len(sent):\n",
    "            group_sent.append(sent[i] + \" \" + sent[i+1])\n",
    "        else:\n",
    "            group_sent.append(sent[i])\n",
    "    if group_sent[-1] == '': group_sent = group_sent[:-1]\n",
    "\n",
    "    return group_sent\n",
    "    \n",
    "# 텍스트 입력\n",
    "def draw_centered_text(img, texts, max_line_length, n):\n",
    "    global space, thick1, thick2\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    font_scale = 2\n",
    "    thickness = thick1\n",
    "    color = (0, 0, 0)\n",
    "    \n",
    "    y0, dy = 300, t_y\n",
    "    width = img.shape[1]\n",
    "\n",
    "    x, y = 0, y0\n",
    "    \n",
    "    space = []\n",
    "\n",
    "    a=-1\n",
    "\n",
    "    for i, line in enumerate(texts[n].split('\\n')):\n",
    "        for j, char in enumerate(line):\n",
    "            a += 1\n",
    "            if j % max_line_length == 0 and j != 0:\n",
    "                y += dy\n",
    "                x = 0\n",
    "\n",
    "            x_offset = (width - max_line_length * t_x) // 2 + x * t_x\n",
    "            y_offset = y\n",
    "            \n",
    "            cv2.putText(img, char, (x_offset, y_offset), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            top_left = (x_offset, y_offset - t_y)\n",
    "            bottom_right = (x_offset + t_x, y_offset)\n",
    "            space.append([top_left, bottom_right])\n",
    "            \n",
    "            x += 1\n",
    "\n",
    "        y += dy\n",
    "        x = 0\n",
    "\n",
    "    return space\n",
    "\n",
    "# bionic reading 기능 구현\n",
    "# 텍스트 입력 (bold)\n",
    "def draw_centered_text_bold(img, texts, max_line_length, n):\n",
    "    global space, thick1, thick2, t_x, t_y, FONT\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    font_scale = 2\n",
    "    thickness = thick1\n",
    "    thickness2 = thick2\n",
    "    color = (0, 0, 0)\n",
    "    \n",
    "    y0, dy = 300, t_y\n",
    "    width = img.shape[1]\n",
    "\n",
    "    x, y = 0, y0\n",
    "    \n",
    "    space = []\n",
    "\n",
    "    a = -1\n",
    "\n",
    "    for i, line in enumerate(texts[n].split('\\n')):\n",
    "        words = line.split(' ')\n",
    "        for word_index, word in enumerate(words):\n",
    "            bold_len = len(word) // 2\n",
    "\n",
    "            for j, char in enumerate(word):\n",
    "                a += 1\n",
    "                if x % max_line_length == 0 and x != 0:\n",
    "                    y += dy\n",
    "                    x = 0\n",
    "\n",
    "                x_offset = (width - max_line_length * t_x) // 2 + x * t_x\n",
    "                y_offset = y\n",
    "\n",
    "                # 각 단어의 앞 n/2개의 알파벳을 볼드 처리\n",
    "                if j < bold_len:\n",
    "                    cv2.putText(img, char, (x_offset, y_offset), font, font_scale, color, thickness2, cv2.LINE_AA)\n",
    "                else:\n",
    "                    cv2.putText(img, char, (x_offset, y_offset), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "                top_left = (x_offset, y_offset - t_y)\n",
    "                bottom_right = (x_offset + t_x, y_offset)\n",
    "                space.append([top_left, bottom_right])\n",
    "                \n",
    "                x += 1\n",
    "\n",
    "            if word_index < len(words) - 1:\n",
    "                a += 1\n",
    "                if x % max_line_length == 0 and x != 0:\n",
    "                    y += dy\n",
    "                    x = 0\n",
    "                x_offset = (width - max_line_length * t_x) // 2 + x * t_x\n",
    "                y_offset = y\n",
    "                space.append([(x_offset, y_offset - t_y), (x_offset + t_x, y_offset)]) \n",
    "                x += 1 \n",
    "\n",
    "        y += dy\n",
    "        x = 0\n",
    "\n",
    "    return space\n",
    "\n",
    "\n",
    "# 시선이 몇 번째 문자 위에 있는지 반환\n",
    "def onText(point, page):\n",
    "    global min_x, min_y, max_x, max_y, max_line_length, texts\n",
    "    if (min_x < point[0] < max_x) & (min_y < point[1] < max_y):\n",
    "        if ((point[1] - min_y) // t_y) * max_line_length + ((point[0] - min_x) // t_x) < len(texts[page]):\n",
    "            return int(((point[1] - min_y) // t_y) * max_line_length + ((point[0] - min_x) // t_x))\n",
    "        else: return -1\n",
    "    else: return -1\n",
    "\n",
    "# 시선이 n번째 문자 근처인지 반환\n",
    "def nearText(n, point):\n",
    "    global space, plus\n",
    "    if n >= len(space): return False\n",
    "    elif space[n][0][0] - plus <= point[0] <= space[n][1][0] + plus:\n",
    "        if space[n][0][1] - plus <= point[1] <= space[n][1][1] + plus:\n",
    "            return True\n",
    "    else: return False\n",
    "    \n",
    "# 텍스트 읽기 관련 파라미터\n",
    "text_file_path = 'text.txt'\n",
    "max_line_length = 22  # 한 줄에 표시할 최대 문자 수\n",
    "text = read_text_file(text_file_path)\n",
    "t_x, t_y = 50, 70 # 텍스트가 차지하는 x길이, y길이\n",
    "len_t = len(text)\n",
    "haveread = [0 for _ in range(len_t)]\n",
    "min_x, max_x, min_y, max_y = 0, 0, 0, 0\n",
    "space = []\n",
    "plus = 200 # 텍스트 주변이라고 인식하는 범위\n",
    "now = -1 # 현재 몇번째 텍스트 읽고 있는지\n",
    "thick1 = 1 # 일반 텍스트 두께\n",
    "thick2 = 5 # 볼드체 두께\n",
    "texts = split_text(text) # 두 문장씩 나누어진 텍스트 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각적 피드백 및 데이터 처리 관련 함수 및 파라미터 (김도윤)\n",
    "#두 점 사이 거리\n",
    "def distance(pts1,pts2):\n",
    "    return int(((pts1[0]-pts2[0])**2+(pts1[1]-pts2[1])**2)**0.5)\n",
    "\n",
    "#원 그리기\n",
    "radius=1\n",
    "cir_list=[[] for _ in range(len(texts))]\n",
    "color=(0,255,0)\n",
    "\n",
    "def circle(point,lst,page):\n",
    "    global radius\n",
    "    found=False\n",
    "    for _ in range(len(lst[page])):\n",
    "        if point in lst[page][_]:\n",
    "            lst[page][_][1]+=0.1            \n",
    "            found=True\n",
    "            break\n",
    "        \n",
    "    if not found:\n",
    "        lst[page].append([point,radius])\n",
    "    \n",
    "\n",
    "def draw_circle(frame,lst,page):\n",
    "    global color\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(len(lst[i])):\n",
    "            cv2.circle(frame,lst[i][j][0],lst[i][j][1],color,-1)\n",
    "draw=0\n",
    "            \n",
    "#time\n",
    "def time_difference_in_hms(seconds1, seconds2):\n",
    "    # 두 시간의 차이를 절대값으로 계산\n",
    "    diff = abs(seconds1 - seconds2)\n",
    "    \n",
    "    # 시, 분, 초로 변환\n",
    "    hours = diff // 3600\n",
    "    minutes = (diff % 3600) // 60\n",
    "    seconds = diff % 60\n",
    "    \n",
    "    return hours, minutes, seconds\n",
    "\n",
    "times=[0,0]\n",
    "sentence_times=[0 for _ in range(len(texts))]\n",
    "\n",
    "def time_difference_in_sec(seconds1,seconds2):\n",
    "    diff = abs(seconds1 - seconds2)\n",
    "    return diff\n",
    "\n",
    "use_time=[time_difference_in_sec(sentence_times[i],sentence_times[i+1]) for i in range(len(texts)-1)]\n",
    "\n",
    "# 그래프 그리기\n",
    "def draw_plot(list_x, list_y):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(list_x, list_y, 'b-')\n",
    "    ax.set_xlabel(\"page\")\n",
    "    ax.set_ylabel(\"time(Sec)\")\n",
    "    \n",
    "    #축 최소 범위 설정\n",
    "    ax.set_xlim(left=1)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True)) #정수만 축에 나타냄\n",
    "    \n",
    "    canvas = FigureCanvas(fig)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # 캔버스를 이미지로 변환\n",
    "    img = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n",
    "    img = img.reshape(canvas.get_width_height()[::-1] + (3,))\n",
    "    \n",
    "    plt.close() #그래프 닫기\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기능 함수 정의 (고민재)\n",
    "# 얼굴 landmark 검출기 객체를 ndarray로 변환\n",
    "def shape_to_np(shape, dtype='int'):\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return coords\n",
    "\n",
    "# 사람이 한 명인지 확인\n",
    "def is_one_person(img, rects):\n",
    "    if len(rects) == 0:\n",
    "        text = \"No Person!\"\n",
    "        text_width, text_height = cv2.getTextSize(text, FONT, FONT_SCALE, FONT_THICKNESS)[0]\n",
    "        text_x = (img.shape[1] - text_width) // 2\n",
    "        text_y = (img.shape[0] - text_height) // 2\n",
    "        cv2.putText(img, text, (text_x, text_y), FONT, FONT_SCALE, (0, 0, 255), FONT_THICKNESS)\n",
    "    elif len(rects) > 1:\n",
    "        text = \"Too Many People!\"\n",
    "        text_width, text_height = cv2.getTextSize(text, FONT, FONT_SCALE, FONT_THICKNESS)[0]\n",
    "        text_x = (img.shape[1] - text_width) // 2\n",
    "        text_y = (img.shape[0] - text_height) // 2\n",
    "        cv2.putText(img, text, (text_x, text_y), FONT, FONT_SCALE, (0, 0, 255), FONT_THICKNESS)\n",
    "    else:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# 눈 위치 마스크 생성\n",
    "def eye_on_mask(mask, side, shape):\n",
    "    points = [shape[i] for i in side]\n",
    "    points = np.array(points, dtype=np.int32)\n",
    "    mask = cv2.fillConvexPoly(mask, points, 255)\n",
    "    return mask\n",
    "\n",
    "# 트랙바 nothing 콜백 함수\n",
    "def nothing(x):\n",
    "    pass\n",
    "\n",
    "# 동공 검출\n",
    "def contouring(thresh, mid, img, right=False, show_pos=False):\n",
    "    cnts, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    try:\n",
    "        cnt = max(cnts, key=cv2.contourArea)\n",
    "        M = cv2.moments(cnt)\n",
    "        cx = int(M['m10'] / M['m00'])\n",
    "        cy = int(M['m01'] / M['m00'])\n",
    "        area = cv2.contourArea(cnt)\n",
    "        if right:\n",
    "            cx += mid\n",
    "        if show_pos:\n",
    "            cv2.circle(img, (cx, cy), 4, (0, 0, 255), 2)\n",
    "        return cx, cy, area\n",
    "    except:\n",
    "        return None, None, None\n",
    "\n",
    "# 모델 입력으로 사용되는 눈 데이터 계산\n",
    "def calculate_eye_data(left_points, right_points, left_pupil, right_pupil):\n",
    "    left_diff = left_points - left_pupil[:2]\n",
    "    right_diff = right_points - right_pupil[:2]\n",
    "    diff = np.hstack((left_diff, right_diff)).flatten()\n",
    "    eye_data = np.hstack((diff, left_pupil[2:3], right_pupil[2:3]))\n",
    "    return eye_data\n",
    "\n",
    "# 눈 데이터로부터 화면 위치 추정\n",
    "def estimate_screen_position(eye_data, num=1, std=3):\n",
    "    eye_data = np.array(eye_data).reshape(1, -1)\n",
    "    if num > 1:\n",
    "        eye_data_batch = np.tile(eye_data, (num, 1))\n",
    "        noise = np.random.normal(0, std, eye_data_batch.shape)\n",
    "        eye_data_batch += noise\n",
    "        screen_positions = model.predict(eye_data_batch)\n",
    "        screen_position = np.mean(screen_positions, axis=0)\n",
    "    else:\n",
    "        screen_position = model.predict(eye_data).flatten()\n",
    "    return screen_position\n",
    "\n",
    "# 매끄러운 움직임 처리\n",
    "def smooth_position(new_position, smoothed_position, alpha=0.5):\n",
    "    if smoothed_position is None:\n",
    "        return new_position\n",
    "    else:\n",
    "        return alpha * new_position + (1 - alpha) * smoothed_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0868\n",
      "Epoch 2/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0624\n",
      "Epoch 3/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0339\n",
      "Epoch 4/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0432\n",
      "Epoch 5/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0270\n",
      "Epoch 6/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0246\n",
      "Epoch 7/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0277\n",
      "Epoch 8/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0231\n",
      "Epoch 9/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0177\n",
      "Epoch 10/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0175\n",
      "Epoch 11/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0194\n",
      "Epoch 12/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0171\n",
      "Epoch 13/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0145\n",
      "Epoch 14/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0146\n",
      "Epoch 15/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0147\n",
      "Epoch 16/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0132\n",
      "Epoch 17/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0107\n",
      "Epoch 18/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0105\n",
      "Epoch 19/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0120\n",
      "Epoch 20/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0111\n",
      "Epoch 21/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0097\n",
      "Epoch 22/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0101\n",
      "Epoch 23/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0108\n",
      "Epoch 24/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0103\n",
      "Epoch 25/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0093\n",
      "Epoch 26/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0092\n",
      "Epoch 27/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0099\n",
      "Epoch 28/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0094\n",
      "Epoch 29/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0087\n",
      "Epoch 30/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0089\n",
      "Epoch 31/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0091\n",
      "Epoch 32/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0087\n",
      "Epoch 33/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0082\n",
      "Epoch 34/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0084\n",
      "Epoch 35/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0085\n",
      "Epoch 36/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0081\n",
      "Epoch 37/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0079\n",
      "Epoch 38/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0080\n",
      "Epoch 39/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0079\n",
      "Epoch 40/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0076\n",
      "Epoch 41/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0075\n",
      "Epoch 42/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0076\n",
      "Epoch 43/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0074\n",
      "Epoch 44/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0072\n",
      "Epoch 45/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.0072\n",
      "Epoch 46/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0071\n",
      "Epoch 47/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0069\n",
      "Epoch 48/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0068\n",
      "Epoch 49/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0068\n",
      "Epoch 50/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0067\n",
      "Epoch 51/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0065\n",
      "Epoch 52/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0065\n",
      "Epoch 53/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0064\n",
      "Epoch 54/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0063\n",
      "Epoch 55/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0062\n",
      "Epoch 56/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0061\n",
      "Epoch 57/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0060\n",
      "Epoch 58/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0060\n",
      "Epoch 59/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0059\n",
      "Epoch 60/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0058\n",
      "Epoch 61/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0057\n",
      "Epoch 62/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0056\n",
      "Epoch 63/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0055\n",
      "Epoch 64/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0055\n",
      "Epoch 65/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0054\n",
      "Epoch 66/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0053\n",
      "Epoch 67/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0052\n",
      "Epoch 68/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0052\n",
      "Epoch 69/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0051\n",
      "Epoch 70/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0050\n",
      "Epoch 71/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0049\n",
      "Epoch 72/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0049\n",
      "Epoch 73/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0048\n",
      "Epoch 74/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0047\n",
      "Epoch 75/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0046\n",
      "Epoch 76/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0046\n",
      "Epoch 77/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0045\n",
      "Epoch 78/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0044\n",
      "Epoch 79/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0044\n",
      "Epoch 80/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0043\n",
      "Epoch 81/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0042\n",
      "Epoch 82/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0042\n",
      "Epoch 83/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0041\n",
      "Epoch 84/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0040\n",
      "Epoch 85/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0040\n",
      "Epoch 86/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0039\n",
      "Epoch 87/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0039\n",
      "Epoch 88/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0038\n",
      "Epoch 89/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0037\n",
      "Epoch 90/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0037\n",
      "Epoch 91/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0036\n",
      "Epoch 92/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0036\n",
      "Epoch 93/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0035\n",
      "Epoch 94/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0035\n",
      "Epoch 95/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0034\n",
      "Epoch 96/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0033\n",
      "Epoch 97/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0033\n",
      "Epoch 98/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0032\n",
      "Epoch 99/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0032\n",
      "Epoch 100/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0031\n",
      "Epoch 101/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0031\n",
      "Epoch 102/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0030\n",
      "Epoch 103/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0030\n",
      "Epoch 104/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0029\n",
      "Epoch 105/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0029\n",
      "Epoch 106/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0029\n",
      "Epoch 107/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0028\n",
      "Epoch 108/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0028\n",
      "Epoch 109/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0027\n",
      "Epoch 110/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0027\n",
      "Epoch 111/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0026\n",
      "Epoch 112/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0026\n",
      "Epoch 113/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0026\n",
      "Epoch 114/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0025\n",
      "Epoch 115/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0025\n",
      "Epoch 116/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0024\n",
      "Epoch 117/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0024\n",
      "Epoch 118/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0024\n",
      "Epoch 119/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0023\n",
      "Epoch 120/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0023\n",
      "Epoch 121/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0023\n",
      "Epoch 122/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0022\n",
      "Epoch 123/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0022\n",
      "Epoch 124/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0022\n",
      "Epoch 125/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0021\n",
      "Epoch 126/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0021\n",
      "Epoch 127/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0021\n",
      "Epoch 128/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0020\n",
      "Epoch 129/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0020\n",
      "Epoch 130/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0020\n",
      "Epoch 131/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0019\n",
      "Epoch 132/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0019\n",
      "Epoch 133/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0019\n",
      "Epoch 134/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.0018\n",
      "Epoch 135/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0018\n",
      "Epoch 136/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0018\n",
      "Epoch 137/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0018\n",
      "Epoch 138/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0017\n",
      "Epoch 139/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0017\n",
      "Epoch 140/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0017\n",
      "Epoch 141/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0017\n",
      "Epoch 142/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0016\n",
      "Epoch 143/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0016\n",
      "Epoch 144/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0016\n",
      "Epoch 145/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0016\n",
      "Epoch 146/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0015\n",
      "Epoch 147/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0015\n",
      "Epoch 148/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0015\n",
      "Epoch 149/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0015\n",
      "Epoch 150/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0015\n",
      "Epoch 151/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0014\n",
      "Epoch 152/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0014\n",
      "Epoch 153/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0014\n",
      "Epoch 154/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0014\n",
      "Epoch 155/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 156/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0013\n",
      "Epoch 157/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0013\n",
      "Epoch 158/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0013\n",
      "Epoch 159/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0013\n",
      "Epoch 160/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - loss: 0.0013\n",
      "Epoch 161/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0012\n",
      "Epoch 162/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0012\n",
      "Epoch 163/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0012\n",
      "Epoch 164/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0012\n",
      "Epoch 165/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0012\n",
      "Epoch 166/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0012\n",
      "Epoch 167/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0011\n",
      "Epoch 168/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0011\n",
      "Epoch 169/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0011\n",
      "Epoch 170/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0011\n",
      "Epoch 171/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0011\n",
      "Epoch 172/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0011\n",
      "Epoch 173/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0011\n",
      "Epoch 174/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0010\n",
      "Epoch 175/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0010\n",
      "Epoch 176/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0010\n",
      "Epoch 177/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0010\n",
      "Epoch 178/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 9.9198e-04\n",
      "Epoch 179/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 9.7931e-04\n",
      "Epoch 180/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 9.6686e-04\n",
      "Epoch 181/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 9.5463e-04\n",
      "Epoch 182/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 9.4261e-04\n",
      "Epoch 183/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 9.3079e-04\n",
      "Epoch 184/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 9.1917e-04\n",
      "Epoch 185/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 9.0775e-04\n",
      "Epoch 186/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 8.9652e-04\n",
      "Epoch 187/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 8.8548e-04\n",
      "Epoch 188/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 8.7462e-04\n",
      "Epoch 189/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 8.6395e-04\n",
      "Epoch 190/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 8.5346e-04\n",
      "Epoch 191/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 8.4314e-04\n",
      "Epoch 192/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 8.3298e-04\n",
      "Epoch 193/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 8.2301e-04\n",
      "Epoch 194/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 8.1318e-04\n",
      "Epoch 195/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 8.0353e-04\n",
      "Epoch 196/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 7.9403e-04\n",
      "Epoch 197/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 7.8468e-04\n",
      "Epoch 198/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 7.7550e-04\n",
      "Epoch 199/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 7.6645e-04\n",
      "Epoch 200/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 7.5756e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idfor\\AppData\\Local\\Temp\\ipykernel_22056\\3037137202.py:69: MatplotlibDeprecationWarning: The tostring_rgb function was deprecated in Matplotlib 3.8 and will be removed two minor releases later. Use buffer_rgba instead.\n",
      "  img = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')\n"
     ]
    }
   ],
   "source": [
    "# 화면 구성\n",
    "# 어느 화면에서나 q를 눌러 프로그램 종료\n",
    "# calibraiton 화면에서는 c를 눌러 데이터 저장\n",
    "# 즉, 버튼 클릭, q, c면 충분한 조작 가능\n",
    "current_screen = \"main_menu\"\n",
    "is_hovered = []\n",
    "\n",
    "# 둥근 버튼 그리는 함수 (https://stackoverflow.com/questions/18973103/how-to-draw-a-rounded-rectangle-rectangle-with-rounded-corners-with-opencv)\n",
    "def rounded_rectangle(src, top_left, bottom_right, radius=0.1, color=255, thickness=1, line_type=cv2.LINE_AA):\n",
    "    p1 = top_left\n",
    "    p3 = bottom_right\n",
    "    p2 = (bottom_right[0], top_left[1])\n",
    "    p4 = (top_left[0], bottom_right[1])\n",
    "\n",
    "    width = abs(p2[0] - p1[0])\n",
    "    height = abs(p4[1] - p1[1])\n",
    "\n",
    "    if radius > 1:\n",
    "        radius = 1\n",
    "\n",
    "    corner_radius = int(radius * min(width, height) / 2)\n",
    "\n",
    "    if thickness < 0:\n",
    "        cv2.rectangle(src, (p1[0] + corner_radius, p1[1]), (p3[0] - corner_radius, p3[1]), color, thickness)\n",
    "        cv2.rectangle(src, (p1[0], p1[1] + corner_radius), (p3[0], p3[1] - corner_radius), color, thickness)\n",
    "\n",
    "        cv2.ellipse(src, (p1[0] + corner_radius, p1[1] + corner_radius), (corner_radius, corner_radius), 180.0, 0, 90, color, thickness, line_type)\n",
    "        cv2.ellipse(src, (p2[0] - corner_radius, p2[1] + corner_radius), (corner_radius, corner_radius), 270.0, 0, 90, color, thickness, line_type)\n",
    "        cv2.ellipse(src, (p3[0] - corner_radius, p3[1] - corner_radius), (corner_radius, corner_radius), 0.0, 0, 90, color, thickness, line_type)\n",
    "        cv2.ellipse(src, (p4[0] + corner_radius, p4[1] - corner_radius), (corner_radius, corner_radius), 90.0, 0, 90, color, thickness, line_type)\n",
    "\n",
    "    cv2.line(src, (p1[0] + corner_radius, p1[1]), (p2[0] - corner_radius, p2[1]), color, abs(thickness), line_type)\n",
    "    cv2.line(src, (p2[0], p2[1] + corner_radius), (p3[0], p3[1] - corner_radius), color, abs(thickness), line_type)\n",
    "    cv2.line(src, (p3[0] - corner_radius, p4[1]), (p4[0] + corner_radius, p4[1]), color, abs(thickness), line_type)\n",
    "    cv2.line(src, (p4[0], p4[1] - corner_radius), (p1[0], p1[1] + corner_radius), color, abs(thickness), line_type)\n",
    "\n",
    "    cv2.ellipse(src, (p1[0] + corner_radius, p1[1] + corner_radius), (corner_radius, corner_radius), 180.0, 0, 90, color, thickness, line_type)\n",
    "    cv2.ellipse(src, (p2[0] - corner_radius, p2[1] + corner_radius), (corner_radius, corner_radius), 270.0, 0, 90, color, thickness, line_type)\n",
    "    cv2.ellipse(src, (p3[0] - corner_radius, p3[1] - corner_radius), (corner_radius, corner_radius), 0.0, 0, 90, color, thickness, line_type)\n",
    "    cv2.ellipse(src, (p4[0] + corner_radius, p4[1] - corner_radius), (corner_radius, corner_radius), 90.0, 0, 90, color, thickness, line_type)\n",
    "\n",
    "    return src\n",
    "\n",
    "# 버튼 그리는 함수\n",
    "def draw_button(image, label, position, color):\n",
    "    x, y = position\n",
    "    rounded_rectangle(image, (x, y), (x + BUTTON_WIDTH, y + BUTTON_HEIGHT), 0.6,color, -1)\n",
    "    text_size = cv2.getTextSize(label, FONT, FONT_SCALE, FONT_THICKNESS)[0]\n",
    "    text_x = x + (BUTTON_WIDTH - text_size[0]) // 2\n",
    "    text_y = y + (BUTTON_HEIGHT + text_size[1]) // 2\n",
    "    cv2.putText(image, label, (text_x, text_y), FONT, FONT_SCALE, TEXT_COLOR, FONT_THICKNESS, LINE_TYPE)\n",
    "\n",
    "# 버튼 클릭 여부 확인\n",
    "def button_click(position, button_position):\n",
    "    x, y = position\n",
    "    bx, by = button_position\n",
    "    return bx <= x <= bx + BUTTON_WIDTH and by <= y <= by + BUTTON_HEIGHT\n",
    "\n",
    "# 버튼 호버 여부 확튼\n",
    "def button_hover(position, button_position):\n",
    "    x, y = position\n",
    "    bx, by = button_position\n",
    "    return bx <= x <= bx + BUTTON_WIDTH and by <= y <= by + BUTTON_HEIGHT\n",
    "\n",
    "# 메인 메뉴 (보정, 텍스트 읽기, 종료)\n",
    "# 이하 다른 메뉴들도 구성은 동일\n",
    "def main_menu(screen):\n",
    "    global current_screen, is_hovered\n",
    "    # 버튼 목록\n",
    "    buttons = [\n",
    "        {\"label\": \"CALIBRATION\", \"position\": (92, 532), \"action\": \"calibration\"},\n",
    "        {\"label\": \"READ TEXT\", \"position\": (492, 532), \"action\": \"read_text_menu\"},\n",
    "        {\"label\": \"EXIT\", \"position\": (892, 532), \"action\": \"exit_program\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    # 마우스 콜백 함수 (버튼 클릭 구현)\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "\n",
    "    main_menu_img=cv2.imread(\"UI-main_menu.png\", cv2.IMREAD_COLOR)\n",
    "    main_menu_img_resize=cv2.resize(main_menu_img,(SCREEN_WIDTH,SCREEN_HEIGHT))\n",
    "    # 메인 메뉴 화면 구현\n",
    "    while current_screen == \"main_menu\":\n",
    "        screen[:] = 0\n",
    "\n",
    "        # 메인 메뉴 이미지 불러오기\n",
    "        screen[0:main_menu_img_resize.shape[0], 0:main_menu_img_resize.shape[1], :] = main_menu_img_resize.copy()\n",
    "\n",
    "        # 버튼 그리기\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "\n",
    "# 보정 메뉴 (landmark, 동공 검출 thresbold, 눈 데이터)\n",
    "def calibration(screen):\n",
    "    global current_screen, is_hovered\n",
    "    buttons = [\n",
    "        {\"label\": \"LANDMARKS\", \"position\": (172, 513), \"action\": \"landmarks_calibration\"},\n",
    "        {\"label\": \"THRESHOLD\", \"position\": (492, 513), \"action\": \"threshold_calibration\"},\n",
    "        {\"label\": \"EYE DATAS\", \"position\": (812, 513), \"action\": \"eye_datas_calibration\"},\n",
    "        {\"label\": \"BACK\", \"position\": (20,20 ), \"action\": \"main_menu\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "\n",
    "    calibration_img=cv2.imread(\"UI-calibration.png\",cv2.IMREAD_COLOR)\n",
    "    calibration_img_resize=cv2.resize(calibration_img,((SCREEN_WIDTH,SCREEN_HEIGHT)))\n",
    "    while current_screen == \"calibration\":\n",
    "        screen[:] = 0\n",
    "\n",
    "        screen[0:calibration_img_resize.shape[0], 0:calibration_img_resize.shape[1], :] = calibration_img_resize.copy()\n",
    "\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "\n",
    "# 텍스트 읽기 메뉴 (테스트, 결과, 뒤로)\n",
    "def read_text_menu(screen):\n",
    "    global current_screen, is_hovered\n",
    "    buttons = [\n",
    "        {\"label\": \"TEST\", \"position\": (250, 513), \"action\": \"test\"},\n",
    "        {\"label\": \"DATA\", \"position\": (700,513), \"action\": \"data\"},\n",
    "        {\"label\": \"BACK\", \"position\": (20, 20), \"action\": \"main_menu\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                if button[\"action\"] == \"Test\": haveread = [0 for i in range(len_t)]\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "\n",
    "    read_text_img=cv2.imread(\"UI-read_text.png\",cv2.IMREAD_COLOR)  \n",
    "    read_text_img_resize=cv2.resize(read_text_img,(SCREEN_WIDTH,SCREEN_HEIGHT))\n",
    "    while current_screen == \"read_text_menu\":\n",
    "        screen[:] = 0\n",
    "\n",
    "        screen[0:read_text_img_resize.shape[0], 0:read_text_img_resize.shape[1], :] = read_text_img_resize.copy()\n",
    "\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "\n",
    "# 텍스트 읽기 테스트 (뒤로, ON)\n",
    "def test(screen):\n",
    "    global current_screen, is_hovered, num, std, alpha, threshold, cap, left, right, kernel, landmarks, min_x, min_y, max_x, max_y, now, haveread\n",
    "    buttons = [\n",
    "        {\"label\": \"ON\", \"position\": (875, 100), \"action\": \"bold_test\"},\n",
    "        {\"label\": \"BACK\", \"position\": (100, 100), \"action\": \"read_text_menu\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen, cap\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                if cap is not None:\n",
    "                    cap.release()\n",
    "                    cap = None\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "    estimated_screen_pos = None\n",
    "    smoothed_screen_pos = None\n",
    "\n",
    "    page = 0\n",
    "\n",
    "    test_img=cv2.imread(\"UI-test.png\",cv2.IMREAD_COLOR)  \n",
    "    test_img_resize=cv2.resize(test_img,(1280,720))\n",
    "    while current_screen == \"test\":\n",
    "        screen[:] = 0\n",
    "        screen[0:test_img_resize.shape[0], 0:test_img_resize.shape[1], :] = test_img_resize.copy()\n",
    "\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        ret, img = cap.read()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 1)\n",
    "        \n",
    "        estimated_screen_pos = None\n",
    "\n",
    "        # 텍스트 입력\n",
    "        space = draw_centered_text(screen, texts, max_line_length, page)\n",
    "        \n",
    "        # 텍스트 영역 범위 변수 저장\n",
    "        min_x, max_x = space[0][0][0], space[min(len(texts[page]),max_line_length)-1][1][0]\n",
    "        min_y, max_y = space[0][0][1], space[len(texts[page])-1][1][1]\n",
    "\n",
    "        global times, sentence_times, draw\n",
    "\n",
    "        # 읽기 시작한 시각과 끝난 시각 저장\n",
    "        if page==0 and now==0:\n",
    "            time_s=time.time()\n",
    "            times[0]=int(time_s)\n",
    "            \n",
    "        elif page==len(texts)-1 and now==len(texts[page])-1:\n",
    "            time_e=time.time()\n",
    "            times[1]=int(time_e)\n",
    "           \n",
    "        \n",
    "        # 각 페이지당 읽은 시간 저장\n",
    "        time_stamp=time.time()\n",
    "        sentence_times[page]=int(time_stamp)\n",
    "\n",
    "        \n",
    "        # 사람이 한 사람일때만 실행\n",
    "        if is_one_person(img,rects):\n",
    "            # 눈 데이터 계산 과정\n",
    "            rect = rects[0]    \n",
    "            shape = predictor(gray, rect)\n",
    "            shape = shape_to_np(shape)\n",
    "            \n",
    "            mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "            mask = eye_on_mask(mask, left, shape)\n",
    "            mask = eye_on_mask(mask, right, shape)\n",
    "            mask = cv2.dilate(mask, kernel, 5)\n",
    "            \n",
    "            eyes = cv2.bitwise_and(img, img, mask=mask)\n",
    "            mask = (eyes == [0, 0, 0]).all(axis=2)\n",
    "            eyes[mask] = [255, 255, 255]\n",
    "            \n",
    "            mid = (shape[42][0] + shape[39][0]) // 2\n",
    "            eyes_gray = cv2.cvtColor(eyes, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            _, thresh = cv2.threshold(eyes_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "            thresh = cv2.erode(thresh, None, iterations=2)\n",
    "            thresh = cv2.dilate(thresh, None, iterations=4)\n",
    "            thresh = cv2.medianBlur(thresh, 3)\n",
    "            thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "            left_pupil = contouring(thresh[:, 0:mid], mid, img, show_pos=True)\n",
    "            right_pupil = contouring(thresh[:, mid:], mid, img, True, show_pos=True)\n",
    "            \n",
    "            left_points = shape[left].copy().astype(np.float32)\n",
    "            right_points = shape[right].copy().astype(np.float32)\n",
    "            \n",
    "            # 눈 데이터를 바탕으로 화면 위치 추정\n",
    "            if left_pupil[0] is not None and right_pupil[0] is not None:\n",
    "                eye_data = calculate_eye_data(left_points, right_points, left_pupil, right_pupil)\n",
    "                estimated_screen_pos = estimate_screen_position(eye_data, num, std)\n",
    "                smoothed_screen_pos = smooth_position(estimated_screen_pos, smoothed_screen_pos, alpha)\n",
    "\n",
    "            \n",
    "        if smoothed_screen_pos is not None:\n",
    "\n",
    "            # 시선 위치 표시\n",
    "            cv2.circle(screen, (int(smoothed_screen_pos[0]*SCREEN_WIDTH), int(smoothed_screen_pos[1]*SCREEN_HEIGHT)), 30, (0, 255, 0), -1)\n",
    "            current_pos = (int(smoothed_screen_pos[0] * SCREEN_WIDTH), int(smoothed_screen_pos[1] * SCREEN_HEIGHT))\n",
    "            word_pos=((space[now][0][0]+space[now][1][0])//2,(space[now][0][1]+space[now][1][1])//2)\n",
    "\n",
    "            # 실시간 피드백 문구 띄우기\n",
    "            if distance(current_pos,word_pos) > 200:\n",
    "                current_pos = np.array(current_pos)\n",
    "                current_pos -= np.array([180,-10])\n",
    "                cv2.putText(screen, \"Return to text\", current_pos, cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)\n",
    "\n",
    "            # 읽고 있는 텍스트 표시\n",
    "            if onText((smoothed_screen_pos[0]*SCREEN_WIDTH,smoothed_screen_pos[1]*SCREEN_HEIGHT),page) != -1:\n",
    "                cv2.rectangle(screen, space[onText((smoothed_screen_pos[0]*SCREEN_WIDTH,smoothed_screen_pos[1]*SCREEN_HEIGHT),page)][0], space[onText((smoothed_screen_pos[0]*SCREEN_WIDTH,smoothed_screen_pos[1]*SCREEN_HEIGHT),page)][1], (0,255,255), -1)\n",
    "\n",
    "            # 지금까지 읽은 텍스트 저장\n",
    "            if nearText(now+1, (smoothed_screen_pos[0]*SCREEN_WIDTH,smoothed_screen_pos[1]*SCREEN_HEIGHT)):\n",
    "                now += 1\n",
    "                haveread[now] = 1\n",
    "            \n",
    "            # 읽은 텍스트 하이라이트\n",
    "            for i in range(len_t):\n",
    "                if haveread[i] > 0:\n",
    "                    cv2.rectangle(screen, space[i][0], space[i][1], (0,255,255), -1)\n",
    "            draw_centered_text(screen, texts, max_line_length, page)\n",
    "\n",
    "\n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # 1 누르면 전 페이지, 2 누르면 다음 페이지\n",
    "        if key == ord('1'):\n",
    "            if page == 0: pass\n",
    "            else: \n",
    "                page -= 1\n",
    "                now = -1\n",
    "                haveread = [0 for i in range(len_t)]\n",
    "\n",
    "        if key == ord('2'):\n",
    "            if page == len(texts) - 1: pass\n",
    "            else: \n",
    "                page += 1\n",
    "                now = -1\n",
    "                haveread = [0 for i in range(len_t)]\n",
    "\n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "    \n",
    "    if cap is not None:\n",
    "        cap.release()\n",
    "        cap = None\n",
    "\n",
    "# 볼드 텍스트 읽기 테스트 (뒤로)\n",
    "def bold_test(screen):\n",
    "    global current_screen, is_hovered, num, std, alpha, threshold, cap, left, right, kernel, landmarks, min_x, min_y, max_x, max_y, now, haveread\n",
    "    buttons = [\n",
    "        {\"label\": \"OFF\", \"position\": (875, 100), \"action\": \"test\"},\n",
    "        {\"label\": \"BACK\", \"position\": (100, 100), \"action\": \"read_text_menu\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen, cap\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                if cap is not None:\n",
    "                    cap.release()\n",
    "                    cap = None\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "    estimated_screen_pos = None\n",
    "    smoothed_screen_pos = None\n",
    "\n",
    "    page = 0\n",
    "\n",
    "    btest_img=cv2.imread(\"UI-test.png\",cv2.IMREAD_COLOR)  \n",
    "    btest_img_resize=cv2.resize(btest_img,(1280,720))\n",
    "    while current_screen == \"bold_test\":\n",
    "        screen[:] = 0\n",
    "        screen[0:btest_img_resize.shape[0], 0:btest_img_resize.shape[1], :] = btest_img_resize.copy()\n",
    "\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        ret, img = cap.read()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 1)\n",
    "        \n",
    "        estimated_screen_pos = None\n",
    "\n",
    "        #텍스트 입력\n",
    "        space = draw_centered_text_bold(screen, texts, max_line_length, page)\n",
    "\n",
    "        # 텍스트 영역 범위 변수 저장\n",
    "        min_x, max_x = space[0][0][0], space[min(len(texts[page]),max_line_length)-1][1][0]\n",
    "        min_y, max_y = space[0][0][1], space[len(texts[page])-1][1][1]     \n",
    "\n",
    "        global times, sentence_times, draw\n",
    "\n",
    "        # 읽기 시작한 시각과 끝난 시각 저장\n",
    "        if page==0 and now==0:\n",
    "            time_s=time.time()\n",
    "            times[0]=int(time_s)\n",
    "            \n",
    "        elif page==len(texts)-1 and now==len(texts[page])-1:\n",
    "            time_e=time.time()\n",
    "            times[1]=int(time_e)\n",
    "        \n",
    "        # 각 페이지당 읽은 시간 저장\n",
    "        time_stamp=time.time()\n",
    "        sentence_times[page]=int(time_stamp)\n",
    "\n",
    "        \n",
    "        # 사람이 한 사람일때만 실행\n",
    "        if is_one_person(img,rects):\n",
    "            # 눈 데이터 계산 과정\n",
    "            rect = rects[0]    \n",
    "            shape = predictor(gray, rect)\n",
    "            shape = shape_to_np(shape)\n",
    "            \n",
    "            mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "            mask = eye_on_mask(mask, left, shape)\n",
    "            mask = eye_on_mask(mask, right, shape)\n",
    "            mask = cv2.dilate(mask, kernel, 5)\n",
    "            \n",
    "            eyes = cv2.bitwise_and(img, img, mask=mask)\n",
    "            mask = (eyes == [0, 0, 0]).all(axis=2)\n",
    "            eyes[mask] = [255, 255, 255]\n",
    "            \n",
    "            mid = (shape[42][0] + shape[39][0]) // 2\n",
    "            eyes_gray = cv2.cvtColor(eyes, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            _, thresh = cv2.threshold(eyes_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "            thresh = cv2.erode(thresh, None, iterations=2)\n",
    "            thresh = cv2.dilate(thresh, None, iterations=4)\n",
    "            thresh = cv2.medianBlur(thresh, 3)\n",
    "            thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "            left_pupil = contouring(thresh[:, 0:mid], mid, img, show_pos=True)\n",
    "            right_pupil = contouring(thresh[:, mid:], mid, img, True, show_pos=True)\n",
    "            \n",
    "            left_points = shape[left].copy().astype(np.float32)\n",
    "            right_points = shape[right].copy().astype(np.float32)\n",
    "            \n",
    "            # 눈 데이터를 바탕으로 화면 위치 추정\n",
    "            if left_pupil[0] is not None and right_pupil[0] is not None:\n",
    "                eye_data = calculate_eye_data(left_points, right_points, left_pupil, right_pupil)\n",
    "                estimated_screen_pos = estimate_screen_position(eye_data, num, std)\n",
    "                smoothed_screen_pos = smooth_position(estimated_screen_pos, smoothed_screen_pos, alpha)                \n",
    "\n",
    "        if smoothed_screen_pos is not None:\n",
    "\n",
    "\n",
    "            current_pos = (int(smoothed_screen_pos[0] * SCREEN_WIDTH), int(smoothed_screen_pos[1] * SCREEN_HEIGHT))\n",
    "            word_pos=((space[now][0][0]+space[now][1][0])//2,(space[now][0][1]+space[now][1][1])//2)\n",
    "\n",
    "            # 시선 위치 표시\n",
    "            cv2.circle(screen, (int(smoothed_screen_pos[0]*SCREEN_WIDTH), int(smoothed_screen_pos[1]*SCREEN_HEIGHT)), 30, (0, 255, 0), -1)\n",
    "            \n",
    "            # 실시간 피드백 문구 띄우기\n",
    "            if distance(current_pos,word_pos) > 200:\n",
    "                current_pos = np.array(current_pos)\n",
    "                current_pos -= np.array([180,-10])\n",
    "                cv2.putText(screen, \"Return to text\", current_pos, FONT, 1.5, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "            # 보고 있는 텍스트 표시\n",
    "            if onText((smoothed_screen_pos[0]*SCREEN_WIDTH,smoothed_screen_pos[1]*SCREEN_HEIGHT),page) != -1:\n",
    "                cv2.rectangle(screen, space[onText((smoothed_screen_pos[0]*SCREEN_WIDTH,smoothed_screen_pos[1]*SCREEN_HEIGHT),page)][0], space[onText((smoothed_screen_pos[0]*SCREEN_WIDTH,smoothed_screen_pos[1]*SCREEN_HEIGHT),page)][1], (0,255,255), -1)\n",
    "\n",
    "            # 마지막으로 읽은 텍스트 저장\n",
    "            if nearText(now+1, (smoothed_screen_pos[0]*SCREEN_WIDTH,smoothed_screen_pos[1]*SCREEN_HEIGHT)):\n",
    "                now += 1\n",
    "                haveread[now] = 1\n",
    "            \n",
    "            # 지금까지 읽은 텍스트 표시\n",
    "            for i in range(len_t):\n",
    "                if haveread[i] > 0:\n",
    "                    cv2.rectangle(screen, space[i][0], space[i][1], (0,255,255), -1)\n",
    "            draw_centered_text_bold(screen, texts, max_line_length, page)\n",
    "\n",
    "\n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # 1 누르면 전 페이지, 2 누르면 다음 페이지\n",
    "        if key == ord('1'):\n",
    "            if page == 0: pass\n",
    "            else: \n",
    "                page -= 1\n",
    "                now = -1\n",
    "                haveread = [0 for i in range(len_t)]\n",
    "\n",
    "        if key == ord('2'):\n",
    "            if page == len(texts) - 1: pass\n",
    "            else: \n",
    "                page += 1\n",
    "                now = -1\n",
    "                haveread = [0 for i in range(len_t)]\n",
    "\n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "    \n",
    "    if cap is not None: \n",
    "        cap.release()\n",
    "        cap = None\n",
    "\n",
    "# 데이터 화면 (뒤로)\n",
    "def data(screen):\n",
    "    global current_screen, is_hovered\n",
    "    buttons = [\n",
    "        {\"label\": \"BACK\", \"position\": (92, 530), \"action\": \"read_text_menu\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "\n",
    "    data_img=cv2.imread(\"UI-data.png\",cv2.IMREAD_COLOR)\n",
    "    data_img_resize=cv2.resize(data_img,(SCREEN_WIDTH,SCREEN_HEIGHT))\n",
    "    while current_screen == \"data\":\n",
    "        screen[:] = 0\n",
    "        screen[0:data_img_resize.shape[0], 0:data_img_resize.shape[1], :] = data_img_resize.copy()\n",
    "\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        # 시간 관련 계산\n",
    "        global times,sentence_times,texts\n",
    "        h,m,s=time_difference_in_hms(times[0],times[1])\n",
    "        if len(texts)==1:\n",
    "            use_time=[sentence_times[0]-times[0]]\n",
    "        else:\n",
    "            use_time=[sentence_times[0]-times[0]]+[time_difference_in_sec(sentence_times[i],sentence_times[i+1]) for i in range(len(texts)-1)]\n",
    "        pages=[i+1 for i in range(len(texts))]\n",
    "        \n",
    "        # 화면에 그래프 표시\n",
    "        plot_img=draw_plot(pages,use_time)\n",
    "        x, y = 540,160\n",
    "        screen[y:y+plot_img.shape[0], x:x+plot_img.shape[1], :] = plot_img.copy()\n",
    "        cv2.putText(screen,f\"TIME READ = {h :02.0f}:{m :02.0f}:{s :02.0f}\",(490,180),FONT,1.5,(0,0,0),3)\n",
    "        \n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        \n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "\n",
    "# landmakrs 보정 화면 (뒤로)   \n",
    "def landmarks_calibration(screen):\n",
    "    global current_screen, is_hovered, cap, landmarks\n",
    "    buttons = [\n",
    "        {\"label\": \"Back\", \"position\": (95, 150), \"action\": \"calibration\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    alert_timer = 0\n",
    "\n",
    "    landmarks_calibration_img=cv2.imread(\"UI-landmarks_calibration.png\",cv2.IMREAD_COLOR)  \n",
    "    landmarks_calibration_img_resize=cv2.resize(landmarks_calibration_img,(SCREEN_WIDTH,SCREEN_HEIGHT))\n",
    "    while current_screen == \"landmarks_calibration\":\n",
    "        screen[:] = 0\n",
    "        screen[0:landmarks_calibration_img_resize.shape[0], 0:landmarks_calibration_img_resize.shape[1], :] = landmarks_calibration_img_resize.copy()\n",
    "\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        # 얼굴 landmark 그리기\n",
    "        ret, img = cap.read()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 1)\n",
    "        if is_one_person(img, rects):\n",
    "            rect = rects[0]    \n",
    "            shape = predictor(gray, rect)\n",
    "            shape = shape_to_np(shape)\n",
    "            \n",
    "            for (x, y) in shape:\n",
    "                cv2.circle(img, (x, y), 2, (255, 0, 0), -1)\n",
    "        \n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        x = 530\n",
    "        y = 150\n",
    "        screen[y:y+img.shape[0], x:x+img.shape[1]] = img\n",
    "        \n",
    "        # c를 누르면 현재 상태의 landmark 저장 (추후에 사용하기 위해)\n",
    "        if key == ord('c') and is_one_person(img, rects):\n",
    "            alert_timer = 20\n",
    "            with open('landmarks.pkl', 'wb') as file:\n",
    "                pickle.dump(shape, file)\n",
    "            with open('landmarks.pkl', 'rb') as file:\n",
    "                landmarks = pickle.load(file)            \n",
    "        \n",
    "        # 저장 알림을 일정 시간동안 표시\n",
    "        if alert_timer > 0:\n",
    "            cv2.putText(screen, \"FACIAL LANDMARK SAVED!\", (640, 620), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "            alert_timer -= 1\n",
    "            \n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        \n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "    if cap is not None:\n",
    "        cap.release()\n",
    "        cap = None\n",
    "\n",
    "# 동공 검출 threshold 보정 화면 (뒤로)\n",
    "def threshold_calibration(screen):\n",
    "    global current_screen, is_hovered, cap, landmarks, kernel, threshold\n",
    "    buttons = [\n",
    "        {\"label\": \"Back\", \"position\": (95, 150), \"action\": \"calibration\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, img = cap.read()\n",
    "    thresh = np.zeros_like(img)\n",
    "    thresh = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 트랙바 사용이 어려워 별도의 화면을 사용하여 threshold 보정\n",
    "    threshold_screen = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    cv2.namedWindow('Threshold', cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Threshold', thresh.shape[1], thresh.shape[0]+30)\n",
    "    cv2.moveWindow('Threshold', 690, 200)\n",
    "    cv2.createTrackbar('threshold', 'Threshold', 0, 255, nothing)\n",
    "    threshold_saved = False\n",
    "\n",
    "    threshold_calibration_img=cv2.imread(\"UI-threshold_calibration.png\",cv2.IMREAD_COLOR)  \n",
    "    threshold_calibration_img_resize=cv2.resize(threshold_calibration_img,(1280,720))\n",
    "    while current_screen == \"threshold_calibration\":\n",
    "        screen[:] = 0\n",
    "\n",
    "        screen[0:threshold_calibration_img_resize.shape[0], 0:threshold_calibration_img_resize.shape[1], :] = threshold_calibration_img_resize.copy()\n",
    "\n",
    "        threshold_screen[:] = 0\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        # threshold가 저장되기 전\n",
    "        if not threshold_saved:\n",
    "            ret, img = cap.read()\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            thresh = np.zeros_like(img)\n",
    "            thresh = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
    "            rects = detector(gray, 1)\n",
    "            for (x, y) in landmarks:\n",
    "                cv2.circle(img, (x, y), 2, (200, 200, 200), -1)\n",
    "            \n",
    "            if is_one_person(img, rects):\n",
    "                # 눈 데이터 계산 과정\n",
    "                rect = rects[0]    \n",
    "                shape = predictor(gray, rect)\n",
    "                shape = shape_to_np(shape)\n",
    "                \n",
    "                mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "                mask = eye_on_mask(mask, left, shape)\n",
    "                mask = eye_on_mask(mask, right, shape)\n",
    "                mask = cv2.dilate(mask, kernel, 5)\n",
    "                \n",
    "                eyes = cv2.bitwise_and(img, img, mask=mask)\n",
    "                mask = (eyes == [0, 0, 0]).all(axis=2)\n",
    "                eyes[mask] = [255, 255, 255]\n",
    "                \n",
    "                eyes_gray = cv2.cvtColor(eyes, cv2.COLOR_BGR2GRAY)\n",
    "                threshold = cv2.getTrackbarPos('threshold', 'Threshold')\n",
    "                \n",
    "                _, thresh = cv2.threshold(eyes_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "                thresh = cv2.erode(thresh, None, iterations=2)\n",
    "                thresh = cv2.dilate(thresh, None, iterations=4)\n",
    "                thresh = cv2.medianBlur(thresh, 3)\n",
    "                thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "                for (x, y) in shape:\n",
    "                    cv2.circle(img, (x, y), 2, (255, 0, 0), -1)\n",
    "            \n",
    "            # threshold에 따른 동공 검출 결과를 색으로 표시\n",
    "            thresh = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
    "            thresh_green = np.zeros_like(thresh)\n",
    "            thresh_green[:, :] = [0, 255, 0]\n",
    "            img = cv2.bitwise_and(img, cv2.bitwise_not(thresh)) + cv2.bitwise_and(thresh_green, thresh)\n",
    "            \n",
    "            threshold_screen[0:img.shape[0], 0:img.shape[1], :] = img\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # c를 누르면 threshold 저를\n",
    "        if key == ord('c') and not threshold_saved:\n",
    "            threshold_saved = True\n",
    "            cap.release()\n",
    "            cv2.destroyWindow('Threshold')\n",
    "        \n",
    "        # 저장되었을 때 화면에 표시\n",
    "        if threshold_saved:\n",
    "            cv2.putText(screen, \"THRESHOLD SAVED!\", (700, 580), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n",
    "        \n",
    "        if not threshold_saved:\n",
    "            cv2.imshow(\"Threshold\", threshold_screen)\n",
    "        \n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "    if cap is not None:\n",
    "        cap.release()\n",
    "    try:\n",
    "        cv2.destroyWindow('Threshold')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 눈 데이터 보정 화면 (보정, 뒤로)\n",
    "def eye_datas_calibration(screen):\n",
    "    global current_screen, is_hovered, cap, landmarks, kernel, threshold, horizontal_number, vertical_number, eye_datas, screen_positions\n",
    "    buttons = [\n",
    "        {\"label\": \"TRAIN\", \"position\": (95, 150), \"action\": \"train\"},\n",
    "        {\"label\": \"BACK\", \"position\": (95, 300), \"action\": \"calibration\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen, cap\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                if cap is not None:\n",
    "                    cap.release()\n",
    "                    cap = None\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # 눈 데이터와 대응되는 화면의 점을 저장하여 모델 학습에 활용\n",
    "    eye_datas = []\n",
    "    screen_positions = []\n",
    "    count = 0\n",
    "\n",
    "    collecting = False\n",
    "\n",
    "    eye_datas_calibration_img=cv2.imread(\"UI-eye_datas_calibration.png\",cv2.IMREAD_COLOR)\n",
    "    eye_datas_calibration_img_resize=cv2.resize(eye_datas_calibration_img,(1280,720))\n",
    "    while current_screen == \"eye_datas_calibration\":\n",
    "        screen[:] = 0\n",
    "\n",
    "        screen[0:eye_datas_calibration_img_resize.shape[0], 0:eye_datas_calibration_img_resize.shape[1], :] = eye_datas_calibration_img_resize.copy()\n",
    "\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "\n",
    "        ret, img = cap.read()\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        rects = detector(gray, 1)\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(img, (x, y), 2, (200, 200, 200), -1)\n",
    "        \n",
    "        if is_one_person(img, rects):\n",
    "            # 눈 데이터 계산 과정\n",
    "            rect = rects[0]    \n",
    "            shape = predictor(gray, rect)\n",
    "            shape = shape_to_np(shape)\n",
    "            \n",
    "            mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "            mask = eye_on_mask(mask, left, shape)\n",
    "            mask = eye_on_mask(mask, right, shape)\n",
    "            mask = cv2.dilate(mask, kernel, 5)\n",
    "            \n",
    "            eyes = cv2.bitwise_and(img, img, mask=mask)\n",
    "            mask = (eyes == [0, 0, 0]).all(axis=2)\n",
    "            eyes[mask] = [255, 255, 255]\n",
    "            \n",
    "            mid = (shape[42][0] + shape[39][0]) // 2\n",
    "            eyes_gray = cv2.cvtColor(eyes, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            _, thresh = cv2.threshold(eyes_gray, threshold, 255, cv2.THRESH_BINARY)\n",
    "            thresh = cv2.erode(thresh, None, iterations=2)\n",
    "            thresh = cv2.dilate(thresh, None, iterations=4)\n",
    "            thresh = cv2.medianBlur(thresh, 3)\n",
    "            thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "            left_pupil = contouring(thresh[:, 0:mid], mid, img, show_pos=True)\n",
    "            right_pupil = contouring(thresh[:, mid:], mid, img, True, show_pos=True)\n",
    "            \n",
    "            left_points = shape[left].copy().astype(np.float32)\n",
    "            right_points = shape[right].copy().astype(np.float32)\n",
    "            \n",
    "            for (x, y) in shape:\n",
    "                cv2.circle(img, (x, y), 2, (255, 0, 0), -1)\n",
    "\n",
    "        x = 530\n",
    "        y = 130\n",
    "            \n",
    "        screen[y:y+img.shape[0], x:x+img.shape[1]] = img\n",
    "        \n",
    "        # 현재 보정에 사용하는 점을 화면에 표시\n",
    "        screen_position = (count % horizontal_number / (horizontal_number-1), count // horizontal_number / (vertical_number-1))\n",
    "        cv2.circle(screen, (int(screen_position[0] * SCREEN_WIDTH), int(screen_position[1] * SCREEN_HEIGHT)), 10, (0, 255, 0), -1)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        # c를 누르면 현재 눈 데이터와 화면 위치 저장\n",
    "        if key == ord('c'):\n",
    "            if not collecting and left_pupil[0] is not None and right_pupil[0] is not None:\n",
    "                collecting = True\n",
    "                screen_positions.append((count % horizontal_number / (horizontal_number-1), count // horizontal_number / (vertical_number-1)))\n",
    "                count += 1\n",
    "                count %= horizontal_number * vertical_number\n",
    "                left_pupil = np.array(left_pupil).astype(np.float32)\n",
    "                right_pupil = np.array(right_pupil).astype(np.float32)\n",
    "                eye_datas.append(calculate_eye_data(left_points, right_points, left_pupil, right_pupil))\n",
    "        else:\n",
    "            collecting = False \n",
    "                   \n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        \n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "    if cap is not None:\n",
    "        cap.release()\n",
    "        cap = None\n",
    "\n",
    "# 눈 데이터로 모델 학습 (뒤로)\n",
    "def train(screen):\n",
    "    global current_screen, is_hovered, cap, landmarks, eye_datas, screen_positions\n",
    "    buttons = [\n",
    "        {\"label\": \"BACK\", \"position\": (92, 530), \"action\": \"calibration\"}\n",
    "    ]\n",
    "    is_hovered = [False] * len(buttons)\n",
    "\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        global current_screen\n",
    "        for i, button in enumerate(buttons):\n",
    "            is_hovered[i] = button_hover((x, y), button[\"position\"])\n",
    "            if event == cv2.EVENT_LBUTTONDOWN and button_click((x, y), button[\"position\"]):\n",
    "                current_screen = button[\"action\"]\n",
    "                return\n",
    "\n",
    "    cv2.setMouseCallback(WINDOW_NAME, mouse_callback)\n",
    "    screen[:] = 0\n",
    "    cv2.imshow(WINDOW_NAME, screen)\n",
    "    \n",
    "    train_timer = 0\n",
    "    train_finished = False\n",
    "    train_img=cv2.imread(\"UI-train.png\",cv2.IMREAD_COLOR)  \n",
    "    train_img_resize=cv2.resize(train_img,(1280,720))\n",
    "    while current_screen == \"train\":\n",
    "        screen[:] = 0\n",
    "\n",
    "        screen[0:train_img_resize.shape[0], 0:train_img_resize.shape[1], :] = train_img_resize.copy()\n",
    "\n",
    "        for i, button in enumerate(buttons):\n",
    "            button_color = BUTTON_COLOR_HOVER if is_hovered[i] else BUTTON_COLOR\n",
    "            draw_button(screen, button[\"label\"], button[\"position\"], button_color)\n",
    "        \n",
    "        # 학습이 완료되었을 때 화면에 표시\n",
    "        if train_finished:\n",
    "            cv2.putText(screen, \"TRAIN FINISHED\", (500, 572), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "        # 눈 데이터와 화면 위치로 모델을 학습 (상황, 개인에 따라 다른 모델을 사용해야 함)\n",
    "        if not train_finished:\n",
    "            train_timer += 1\n",
    "            cv2.putText(screen, \"TRAINING...\", (500, 572), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            if train_timer == 10:\n",
    "                train_finished = True\n",
    "                X = np.array(eye_datas)\n",
    "                y = np.array(screen_positions)\n",
    "                model.compile(optimizer='adam', loss='mse')\n",
    "                model.fit(X, y, epochs=200, batch_size=32)\n",
    "            \n",
    "        cv2.imshow(WINDOW_NAME, screen)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            exit_program()\n",
    "    if cap is not None:\n",
    "        cap.release()\n",
    "        cap = None\n",
    "\n",
    "# 프로그램 종료 (화면 종료, 카메라 해제)\n",
    "def exit_program():\n",
    "    global current_screen, cap\n",
    "    current_screen = None\n",
    "    cv2.destroyAllWindows()\n",
    "    if cap is not None:\n",
    "        cap.release()\n",
    "        cap = None\n",
    "\n",
    "# 화면 관리 함수 \n",
    "if __name__ == \"__main__\":\n",
    "    screen = np.zeros((SCREEN_HEIGHT, SCREEN_WIDTH, 3), dtype=np.uint8)\n",
    "    cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(WINDOW_NAME, SCREEN_WIDTH, SCREEN_HEIGHT)\n",
    "    cv2.setWindowProperty(WINDOW_NAME, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n",
    "    while current_screen:\n",
    "        if current_screen == \"main_menu\":\n",
    "            main_menu(screen)\n",
    "        elif current_screen == \"calibration\":\n",
    "            calibration(screen)\n",
    "        elif current_screen == \"read_text_menu\":\n",
    "            read_text_menu(screen)\n",
    "        elif current_screen == \"test\":\n",
    "            test(screen)\n",
    "        elif current_screen == \"bold_test\":\n",
    "            bold_test(screen)\n",
    "        elif current_screen == \"data\":\n",
    "            data(screen)\n",
    "        elif current_screen == \"landmarks_calibration\":\n",
    "            landmarks_calibration(screen)\n",
    "        elif current_screen == \"threshold_calibration\":\n",
    "            threshold_calibration(screen)\n",
    "        elif current_screen == \"eye_datas_calibration\":\n",
    "            eye_datas_calibration(screen)\n",
    "        elif current_screen == \"train\":\n",
    "            train(screen)\n",
    "        elif current_screen == \"exit_program\":\n",
    "            exit_program()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튕겼을 때 실행\n",
    "try:\n",
    "    cap.release()\n",
    "except:\n",
    "    pass\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
